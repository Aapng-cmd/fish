{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2/9eURqArBzsjbPKOUVl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aapng-cmd/fish/blob/master/voice_synth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "CVK3cHl7Gwd4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "qlUQt_Fq8DZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# nltk.download('punkt')  # download the Punkt tokenizer models\n",
        "\n",
        "def transcribe_audio(path):\n",
        "    with sr.AudioFile(path) as source:\n",
        "        audio_listened = r.record(source)\n",
        "        try:\n",
        "            text = r.recognize_google(audio_listened, language=\"ru-RU\")\n",
        "        except sr.UnknownValueError as e:\n",
        "            print(\"Error:\", str(e))\n",
        "            text = \"\"\n",
        "        return text\n",
        "\n",
        "def get_large_audio_transcription_on_silence(path):\n",
        "    sound = AudioSegment.from_file(path)\n",
        "    chunks = split_on_silence(sound, min_silence_len=500, silence_thresh=sound.dBFS-14, keep_silence=500)\n",
        "    folder_name = \"audio-chunks\"\n",
        "    if not os.path.isdir(folder_name):\n",
        "        os.mkdir(folder_name)\n",
        "    whole_text = \"\"\n",
        "    for i, audio_chunk in enumerate(chunks, start=1):\n",
        "        chunk_filename = os.path.join(folder_name, f\"chunk{i}.wav\")\n",
        "        audio_chunk.export(chunk_filename, format=\"wav\")\n",
        "        text = transcribe_audio(chunk_filename)\n",
        "        whole_text += text + \" \"\n",
        "    sentences = sent_tokenize(whole_text)\n",
        "    return \". \".join(sentences)\n",
        "\n",
        "r = sr.Recognizer()\n",
        "for i in range(1):\n",
        "    path = f\"audio/sample{i}.wav\"\n",
        "    text = get_large_audio_transcription_on_silence(path)\n",
        "    with open(f\"data/sample{i}.txt\", \"w\") as f:\n",
        "        f.write(text)\n",
        "\n",
        "    print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc6MV4fk8gfS",
        "outputId": "e7dcd66f-044e-4d47-c06f-969da68b76ab"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "подушка кровать корова наушники футболка телевизор приставка\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, audio_files, text_files):\n",
        "        self.audio_files = audio_files\n",
        "        self.text_files = text_files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_file = self.audio_files[idx]\n",
        "        text_file = self.text_files[idx]\n",
        "\n",
        "        # Load audio file with error handling\n",
        "        try:\n",
        "            audio, sr = librosa.load(audio_file)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading audio file: {audio_file}, {e}\")\n",
        "            audio = torch.zeros(1, 80)  # Replace with a placeholder if loading fails\n",
        "            sr = 22050  # Default sample rate\n",
        "\n",
        "        # Extract MFCC features from audio data\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=80)\n",
        "        mfccs = mfccs.T  # Transpose to (time_steps, features)\n",
        "\n",
        "        # Load text transcription\n",
        "        with open(text_file, 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Tokenize text\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Convert tokens to numerical IDs (you'll need a vocabulary for this)\n",
        "        vocab = {'<pad>': 0, '<unk>': 1}  # example vocabulary\n",
        "        token_ids = [vocab.get(token, 1) for token in tokens]\n",
        "\n",
        "        # Preprocess audio and text\n",
        "        audio_tensor = torch.tensor(mfccs)  # (time_steps, features)\n",
        "        text_tensor = torch.tensor(token_ids)\n",
        "\n",
        "        return audio_tensor, text_tensor"
      ],
      "metadata": {
        "id": "2fpQxLICG2H7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MyDataset([f'audio/sample{i}.wav' for i in range(1)], [f'data/sample{i}.txt' for i in range(1)])\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "UvA0FcwkG426"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastSpeech(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FastSpeech, self).__init__()\n",
        "        self.encoder = nn.LSTM(input_size=80, hidden_size=256, num_layers=2, batch_first=True)\n",
        "        self.decoder = nn.Linear(256, 7)  # Output dimension matches the shape of the text tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.encoder(x)\n",
        "        x = self.decoder(x[:, -1, :])  # Take the last hidden state and pass it through the decoder\n",
        "        return x"
      ],
      "metadata": {
        "id": "NJLsGv9VG_GZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastSpeech()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for batch in data_loader:\n",
        "        audio, text = batch\n",
        "        audio = audio.to(device)\n",
        "        text = text.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(audio)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output, text.float())\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Reset the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Print the loss for the current batch\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "id": "g8bVgQuqHCPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a15cc688-765d-4865-fae8-bb624037a26e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.0086119174957275\n",
            "Epoch 2, Loss: 0.6742094159126282\n",
            "Epoch 3, Loss: 0.4082925319671631\n",
            "Epoch 4, Loss: 0.14254361391067505\n",
            "Epoch 5, Loss: 0.019336888566613197\n",
            "Epoch 6, Loss: 0.0608539916574955\n",
            "Epoch 7, Loss: 0.09472382813692093\n",
            "Epoch 8, Loss: 0.07359181344509125\n",
            "Epoch 9, Loss: 0.03889273852109909\n",
            "Epoch 10, Loss: 0.016895201057195663\n",
            "Epoch 11, Loss: 0.010884399525821209\n",
            "Epoch 12, Loss: 0.013557950966060162\n",
            "Epoch 13, Loss: 0.01785932667553425\n",
            "Epoch 14, Loss: 0.020316969603300095\n",
            "Epoch 15, Loss: 0.020191740244627\n",
            "Epoch 16, Loss: 0.018096206709742546\n",
            "Epoch 17, Loss: 0.015030339360237122\n",
            "Epoch 18, Loss: 0.011885138228535652\n",
            "Epoch 19, Loss: 0.009230030700564384\n",
            "Epoch 20, Loss: 0.00728000421077013\n",
            "Epoch 21, Loss: 0.005984848830848932\n",
            "Epoch 22, Loss: 0.0051641869358718395\n",
            "Epoch 23, Loss: 0.004624220542609692\n",
            "Epoch 24, Loss: 0.004236070904880762\n",
            "Epoch 25, Loss: 0.0039472379721701145\n",
            "Epoch 26, Loss: 0.0037458795122802258\n",
            "Epoch 27, Loss: 0.0036188450176268816\n",
            "Epoch 28, Loss: 0.003527991473674774\n",
            "Epoch 29, Loss: 0.0034137044567614794\n",
            "Epoch 30, Loss: 0.00321779353544116\n",
            "Epoch 31, Loss: 0.002914279233664274\n",
            "Epoch 32, Loss: 0.002531809499487281\n",
            "Epoch 33, Loss: 0.0020606908947229385\n",
            "Epoch 34, Loss: 0.0015958646545186639\n",
            "Epoch 35, Loss: 0.0012333985650911927\n",
            "Epoch 36, Loss: 0.0010045486269518733\n",
            "Epoch 37, Loss: 0.0009144255891442299\n",
            "Epoch 38, Loss: 0.0009342533885501325\n",
            "Epoch 39, Loss: 0.0010110634611919522\n",
            "Epoch 40, Loss: 0.0010851325932890177\n",
            "Epoch 41, Loss: 0.0011076446389779449\n",
            "Epoch 42, Loss: 0.0010532799642533064\n",
            "Epoch 43, Loss: 0.0009243756649084389\n",
            "Epoch 44, Loss: 0.0007463946822099388\n",
            "Epoch 45, Loss: 0.000557275372557342\n",
            "Epoch 46, Loss: 0.00039482556167058647\n",
            "Epoch 47, Loss: 0.00028602912789210677\n",
            "Epoch 48, Loss: 0.00024075967667158693\n",
            "Epoch 49, Loss: 0.0002502910792827606\n",
            "Epoch 50, Loss: 0.00029237885610200465\n",
            "Epoch 51, Loss: 0.00034025590866804123\n",
            "Epoch 52, Loss: 0.0003700589877553284\n",
            "Epoch 53, Loss: 0.00036629228270612657\n",
            "Epoch 54, Loss: 0.000325988105032593\n",
            "Epoch 55, Loss: 0.00025860682944767177\n",
            "Epoch 56, Loss: 0.00018185304361395538\n",
            "Epoch 57, Loss: 0.0001152291297330521\n",
            "Epoch 58, Loss: 7.349918450927362e-05\n",
            "Epoch 59, Loss: 6.216872134245932e-05\n",
            "Epoch 60, Loss: 7.644340803381056e-05\n",
            "Epoch 61, Loss: 0.00010381663014413789\n",
            "Epoch 62, Loss: 0.00012913781392853707\n",
            "Epoch 63, Loss: 0.0001401879417244345\n",
            "Epoch 64, Loss: 0.00013166421558707952\n",
            "Epoch 65, Loss: 0.00010625780851114541\n",
            "Epoch 66, Loss: 7.268816989380866e-05\n",
            "Epoch 67, Loss: 4.172727858531289e-05\n",
            "Epoch 68, Loss: 2.1930871298536658e-05\n",
            "Epoch 69, Loss: 1.6739051716285758e-05\n",
            "Epoch 70, Loss: 2.3878175852587447e-05\n",
            "Epoch 71, Loss: 3.700915112858638e-05\n",
            "Epoch 72, Loss: 4.870144039159641e-05\n",
            "Epoch 73, Loss: 5.33969905518461e-05\n",
            "Epoch 74, Loss: 4.9221853259950876e-05\n",
            "Epoch 75, Loss: 3.809220652328804e-05\n",
            "Epoch 76, Loss: 2.436163413221948e-05\n",
            "Epoch 77, Loss: 1.2732819413940888e-05\n",
            "Epoch 78, Loss: 6.397538982128026e-06\n",
            "Epoch 79, Loss: 6.0574893723241985e-06\n",
            "Epoch 80, Loss: 1.007327409752179e-05\n",
            "Epoch 81, Loss: 1.548642285342794e-05\n",
            "Epoch 82, Loss: 1.937594970513601e-05\n",
            "Epoch 83, Loss: 1.9972136215073988e-05\n",
            "Epoch 84, Loss: 1.7140977433882654e-05\n",
            "Epoch 85, Loss: 1.2156217962910887e-05\n",
            "Epoch 86, Loss: 6.961926828807918e-06\n",
            "Epoch 87, Loss: 3.294026100775227e-06\n",
            "Epoch 88, Loss: 2.0244542611180805e-06\n",
            "Epoch 89, Loss: 2.94994219984801e-06\n",
            "Epoch 90, Loss: 5.043625151301967e-06\n",
            "Epoch 91, Loss: 6.998322987783467e-06\n",
            "Epoch 92, Loss: 7.803038897691295e-06\n",
            "Epoch 93, Loss: 7.112171715561999e-06\n",
            "Epoch 94, Loss: 5.285608040139778e-06\n",
            "Epoch 95, Loss: 3.1252218377630925e-06\n",
            "Epoch 96, Loss: 1.4613968915000441e-06\n",
            "Epoch 97, Loss: 7.877776511122647e-07\n",
            "Epoch 98, Loss: 1.0953887112918892e-06\n",
            "Epoch 99, Loss: 1.953282890099217e-06\n",
            "Epoch 100, Loss: 2.768331341940211e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "torch.save(model.state_dict(), 'models/fastspeech.pt')"
      ],
      "metadata": {
        "id": "6oC4Oxhl7ljq"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}